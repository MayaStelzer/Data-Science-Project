{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "# Mushroom Classification\n\n## Dataset (mushrooms.csv)\n- Size: 8124 instances\n- Features: 22 categorical features in the original dataset\n- Target: class (edible or poisonous)\n\n### Analysis Uses Simplified Feature Set:\n9 selected features (simplified to at most 3 categories each):\n1. cap-color: gray, brown, other\n2. odor: foul, none, other\n3. stalk-surface-above-ring: smooth, silky, other\n4. stalk-surface-below-ring: smooth, silky, other\n5. stalk-color-above-ring: pink, white, other\n6. stalk-color-below-ring: pink, white, other\n7. ring-type: pendant, evanescent, other\n8. population: several, other\n9. habitat: wood, grass, other"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 8124 instances\n",
      "Columns: ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat']\n"
     ]
    }
   ],
   "source": [
    "# Load the mushroom dataset\n",
    "mushroom_table = MyPyTable()\n",
    "mushroom_table.load_from_file('mushrooms.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {len(mushroom_table.data)} instances\")\n",
    "print(f\"Columns: {mushroom_table.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run 10-fold cross-validation and display results\n",
    "def evaluate_feature_subset(X, y, subset_name, feature_names):\n",
    "    \"\"\"Run 10-fold cross-validation and compute metrics for feature subset.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{subset_name}\")\n",
    "    print(f\"Features: {', '.join(feature_names)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Perform 10-fold cross-validation\n",
    "    folds = myevaluation.kfold_split(X, n_splits=10, random_state=42, shuffle=True)\n",
    "    \n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    \n",
    "    for train_indices, test_indices in folds:\n",
    "        # Split data\n",
    "        X_train = [X[i] for i in train_indices]\n",
    "        y_train = [y[i] for i in train_indices]\n",
    "        X_test = [X[i] for i in test_indices]\n",
    "        y_test = [y[i] for i in test_indices]\n",
    "        \n",
    "        # Train and predict\n",
    "        tree = MyDecisionTreeClassifier()\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "        \n",
    "        all_y_true.extend(y_test)\n",
    "        all_y_pred.extend(y_pred)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = myevaluation.accuracy_score(all_y_true, all_y_pred)\n",
    "    error_rate = 1 - accuracy\n",
    "    \n",
    "   \n",
    "    labels = sorted(list(set(all_y_true)))\n",
    "    print(f\"Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Error Rate:  {error_rate:.4f} ({error_rate*100:.2f}%)\")\n",
    "    \n",
    "    # Per class metrics\n",
    "    print(f\"\\nPer-Class Performance Metrics:\")\n",
    "    print(f\"{'Class':<15} {'Precision':>12} {'Recall':>12} {'F1-Score':>12}\")\n",
    "    print(\"-\" * 53)\n",
    "    \n",
    "    for label in labels:\n",
    "        precision = myevaluation.binary_precision_score(all_y_true, all_y_pred, label)\n",
    "        recall = myevaluation.binary_recall_score(all_y_true, all_y_pred, label)\n",
    "        f1 = myevaluation.binary_f1_score(all_y_true, all_y_pred, label)\n",
    "        print(f\"{label:<15} {precision:>12.4f} {recall:>12.4f} {f1:>12.4f}\")\n",
    "    \n",
    "    # Confusion\n",
    "    conf_matrix = myevaluation.confusion_matrix(all_y_true, all_y_pred, labels)\n",
    "    myutils.print_confusion_matrix(conf_matrix, labels, \"\\nConfusion Matrix\")\n",
    "    \n",
    "    return accuracy, tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 1: Using Only the Odor Feature\n",
    "\n",
    "This is a baseline set of results showing how accurately the model can predict whether a mushroom is edible or poisonous using only the odor feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Odor Only\n",
      "Features: odor\n",
      "================================================================================\n",
      "Accuracy:    0.9852 (98.52%)\n",
      "Error Rate:  0.0148 (1.48%)\n",
      "\n",
      "Per-Class Performance Metrics:\n",
      "Class              Precision       Recall     F1-Score\n",
      "-----------------------------------------------------\n",
      "e                     0.9723       1.0000       0.9859\n",
      "p                     1.0000       0.9694       0.9844\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "----------------------------------------\n",
      "                        e         p | Total\n",
      "----------------------------------------\n",
      "              e      4208         0 | 4208\n",
      "              p       120      3796 | 3916\n",
      "----------------------------------------\n",
      "          Total      4328      3796 | 8124\n"
     ]
    }
   ],
   "source": [
    "odor_idx = mushroom_table.column_names.index('odor')\n",
    "label_idx = mushroom_table.column_names.index('class')\n",
    "\n",
    "X_odor = [[row[odor_idx]] for row in mushroom_table.data]\n",
    "y = [row[label_idx] for row in mushroom_table.data]\n",
    "\n",
    "acc_odor, tree_odor = evaluate_feature_subset(X_odor, y, \"Odor Only\", ['odor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Multiple Feature Subsets\n",
    "\n",
    "Testing 4 different feature subsets to find the best combination for predicting mushroom edibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Subset 1: Odor and Cap Color\n",
      "Features: odor, cap-color\n",
      "================================================================================\n",
      "Accuracy:    0.9882 (98.82%)\n",
      "Error Rate:  0.0118 (1.18%)\n",
      "\n",
      "Per-Class Performance Metrics:\n",
      "Class              Precision       Recall     F1-Score\n",
      "-----------------------------------------------------\n",
      "e                     0.9777       1.0000       0.9887\n",
      "p                     1.0000       0.9755       0.9876\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "----------------------------------------\n",
      "                        e         p | Total\n",
      "----------------------------------------\n",
      "              e      4208         0 | 4208\n",
      "              p        96      3820 | 3916\n",
      "----------------------------------------\n",
      "          Total      4304      3820 | 8124\n",
      "\n",
      "================================================================================\n",
      "Subset 2: Odor and Stalk Features\n",
      "Features: odor, stalk-surface-above-ring, stalk-color-above-ring\n",
      "================================================================================\n",
      "Accuracy:    0.9862 (98.62%)\n",
      "Error Rate:  0.0138 (1.38%)\n",
      "\n",
      "Per-Class Performance Metrics:\n",
      "Class              Precision       Recall     F1-Score\n",
      "-----------------------------------------------------\n",
      "e                     0.9741       1.0000       0.9869\n",
      "p                     1.0000       0.9714       0.9855\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "----------------------------------------\n",
      "                        e         p | Total\n",
      "----------------------------------------\n",
      "              e      4208         0 | 4208\n",
      "              p       112      3804 | 3916\n",
      "----------------------------------------\n",
      "          Total      4320      3804 | 8124\n",
      "\n",
      "================================================================================\n",
      "Subset 3: Odor and Ring and Habitat\n",
      "Features: odor, habitat\n",
      "================================================================================\n",
      "Accuracy:    0.9897 (98.97%)\n",
      "Error Rate:  0.0103 (1.03%)\n",
      "\n",
      "Per-Class Performance Metrics:\n",
      "Class              Precision       Recall     F1-Score\n",
      "-----------------------------------------------------\n",
      "e                     0.9804       1.0000       0.9901\n",
      "p                     1.0000       0.9785       0.9892\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "----------------------------------------\n",
      "                        e         p | Total\n",
      "----------------------------------------\n",
      "              e      4208         0 | 4208\n",
      "              p        84      3832 | 3916\n",
      "----------------------------------------\n",
      "          Total      4292      3832 | 8124\n",
      "\n",
      "================================================================================\n",
      "Subset 4: Comprehensive (5 features)\n",
      "Features: odor, cap-color, stalk-color-below-ring, population, habitat\n",
      "================================================================================\n",
      "Accuracy:    1.0000 (100.00%)\n",
      "Error Rate:  0.0000 (0.00%)\n",
      "\n",
      "Per-Class Performance Metrics:\n",
      "Class              Precision       Recall     F1-Score\n",
      "-----------------------------------------------------\n",
      "e                     1.0000       1.0000       1.0000\n",
      "p                     1.0000       1.0000       1.0000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "----------------------------------------\n",
      "                        e         p | Total\n",
      "----------------------------------------\n",
      "              e      4208         0 | 4208\n",
      "              p         0      3916 | 3916\n",
      "----------------------------------------\n",
      "          Total      4208      3916 | 8124\n"
     ]
    }
   ],
   "source": [
    "# feature subsets\n",
    "feature_subsets = [\n",
    "    {\n",
    "        'name': 'Subset 1: Odor and Cap Color',\n",
    "        'features': ['odor', 'cap-color']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Subset 2: Odor and Stalk Features',\n",
    "        'features': ['odor', 'stalk-surface-above-ring', 'stalk-color-above-ring']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Subset 3: Odor and Ring and Habitat',\n",
    "        'features': ['odor', 'habitat']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Subset 4: Comprehensive (5 features)',\n",
    "        'features': ['odor', 'cap-color', 'stalk-color-below-ring', 'population', 'habitat']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Store results to compare\n",
    "results = [{'name': 'Odor Only', 'accuracy': acc_odor, 'features': ['odor']}]\n",
    "\n",
    "for subset in feature_subsets:\n",
    "    # Get feature indices\n",
    "    feature_indices = [mushroom_table.column_names.index(f) for f in subset['features']]\n",
    "    \n",
    "    # get features\n",
    "    X_subset = [[row[idx] for idx in feature_indices] for row in mushroom_table.data]\n",
    "    \n",
    "    # Evaluate\n",
    "    acc, tree = evaluate_feature_subset(X_subset, y, subset['name'], subset['features'])\n",
    "    results.append({'name': subset['name'], 'accuracy': acc, 'features': subset['features'], 'tree': tree})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Subset                                        # Features     Accuracy   Error Rate\n",
      "--------------------------------------------------------------------------------\n",
      "Odor Only                                              1       0.9852       0.0148\n",
      "Subset 1: Odor and Cap Color                           2       0.9882       0.0118\n",
      "Subset 2: Odor and Stalk Features                      3       0.9862       0.0138\n",
      "Subset 3: Odor and Ring and Habitat                    2       0.9897       0.0103\n",
      "Subset 4: Comprehensive (5 features)                   5       1.0000       0.0000\n",
      "\n",
      "================================================================================\n",
      "BEST SUBSET\n",
      "================================================================================\n",
      "Name:      Subset 4: Comprehensive (5 features)\n",
      "Features:  odor, cap-color, stalk-color-below-ring, population, habitat\n",
      "Accuracy:  1.0000 (100.00%)\n",
      "Error:     0.0000 (0.00%)\n",
      "================================================================================\n",
      "\n",
      "Improvement over baseline (Odor Only): +0.0148 (1.48 percentage points)\n"
     ]
    }
   ],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"{'Subset':<45} {'# Features':>10} {'Accuracy':>12} {'Error Rate':>12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for result in results:\n",
    "    error_rate = 1 - result['accuracy']\n",
    "    print(f\"{result['name']:<45} {len(result['features']):>10} \"\n",
    "          f\"{result['accuracy']:>12.4f} {error_rate:>12.4f}\")\n",
    "\n",
    "# Find best subset\n",
    "best_result = max(results, key=lambda x: x['accuracy'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST SUBSET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Name:      {best_result['name']}\")\n",
    "print(f\"Features:  {', '.join(best_result['features'])}\")\n",
    "print(f\"Accuracy:  {best_result['accuracy']:.4f} ({best_result['accuracy']*100:.2f}%)\")\n",
    "print(f\"Error:     {(1-best_result['accuracy']):.4f} ({(1-best_result['accuracy'])*100:.2f}%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show improvement over baseline\n",
    "baseline_acc = results[0]['accuracy']\n",
    "improvement = best_result['accuracy'] - baseline_acc\n",
    "if improvement > 0:\n",
    "    print(f\"\\nImprovement over baseline (Odor Only): +{improvement:.4f} \"\n",
    "          f\"({improvement*100:.2f} percentage points)\")\n",
    "elif improvement < 0:\n",
    "    print(f\"\\nChange from baseline (Odor Only): {improvement:.4f} \"\n",
    "          f\"({improvement*100:.2f} percentage points)\")\n",
    "else:\n",
    "    print(f\"\\nPerformance equals baseline (Odor Only)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Decision Rules from Best Model\n",
    "\n",
    "Training the best feature subset on the entire dataset to extract decision rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DECISION RULES FROM BEST MODEL\n",
      "================================================================================\n",
      "\n",
      "Features used: ['odor', 'cap-color', 'stalk-color-below-ring', 'population', 'habitat']\n",
      "\n",
      "IF odor == a THEN label = e\n",
      "IF odor == c THEN label = p\n",
      "IF odor == f THEN label = p\n",
      "IF odor == l THEN label = e\n",
      "IF odor == m THEN label = p\n",
      "IF odor == n AND cap-color == b AND population == a THEN label = e\n",
      "IF odor == n AND cap-color == b AND population == c THEN label = e\n",
      "IF odor == n AND cap-color == b AND population == n THEN label = e\n",
      "IF odor == n AND cap-color == b AND population == s THEN label = e\n",
      "IF odor == n AND cap-color == b AND population == v THEN label = p\n",
      "IF odor == n AND cap-color == b AND population == y THEN label = e\n",
      "IF odor == n AND cap-color == c THEN label = e\n",
      "IF odor == n AND cap-color == e THEN label = e\n",
      "IF odor == n AND cap-color == g THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == b THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == c THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == e THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == g THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == a THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == c THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == n THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == s THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == v THEN label = p\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == y THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == g THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == l THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == m THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == p THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == u THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == w THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == o THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == p THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == w THEN label = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == y THEN label = p\n",
      "IF odor == n AND cap-color == p AND habitat == d THEN label = e\n",
      "IF odor == n AND cap-color == p AND habitat == g THEN label = p\n",
      "IF odor == n AND cap-color == p AND habitat == l THEN label = e\n",
      "IF odor == n AND cap-color == p AND habitat == m THEN label = p\n",
      "IF odor == n AND cap-color == p AND habitat == p THEN label = e\n",
      "IF odor == n AND cap-color == p AND habitat == u THEN label = e\n",
      "IF odor == n AND cap-color == p AND habitat == w THEN label = e\n",
      "IF odor == n AND cap-color == r THEN label = e\n",
      "IF odor == n AND cap-color == u THEN label = e\n",
      "IF odor == n AND cap-color == w AND population == a THEN label = e\n",
      "IF odor == n AND cap-color == w AND population == c THEN label = p\n",
      "IF odor == n AND cap-color == w AND population == n THEN label = e\n",
      "IF odor == n AND cap-color == w AND population == s THEN label = e\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == d THEN label = e\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == g THEN label = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == l THEN label = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == m THEN label = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == p THEN label = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == u THEN label = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == w THEN label = p\n",
      "IF odor == n AND cap-color == w AND population == y THEN label = e\n",
      "IF odor == n AND cap-color == y THEN label = p\n",
      "IF odor == p THEN label = p\n",
      "IF odor == s THEN label = p\n",
      "IF odor == y THEN label = p\n"
     ]
    }
   ],
   "source": [
    "# Train best model on entire dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DECISION RULES FROM BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFeatures used: {best_result['features']}\\n\")\n",
    "\n",
    "# Get feature indices for best subset\n",
    "best_feature_indices = [mushroom_table.column_names.index(f) for f in best_result['features']]\n",
    "X_best = [[row[idx] for idx in best_feature_indices] for row in mushroom_table.data]\n",
    "\n",
    "# Train on entire dataset\n",
    "final_tree = MyDecisionTreeClassifier()\n",
    "final_tree.fit(X_best, y)\n",
    "\n",
    "# Print decision rules\n",
    "final_tree.print_decision_rules(attribute_names=best_result['features'], class_name='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Analysis and Pruning Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "- Rules with very few instances are not very applicable and can be pruned and replaced with majority class\n",
      "- Multiple rules leading to the same class with similar conditions can be merged them\n",
      "- For each internal node, if removing that subtree and replacing with majority class doesn't affect accuracy on validation set it can be pruned\n",
      "- Leaf nodes with low confidence can be replace with parent's majority or mark as uncertain\n",
      "- The most important features can be kept and rules that split on less important features in the tree can be pruned\n",
      "- Pruning decisions should be based on: validation set results, instance counts at each node, classification confidence for leaf nodes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "- Rules with very few instances are not very applicable and can be pruned and replaced with majority class\n",
    "- Multiple rules leading to the same class with similar conditions can be merged them\n",
    "- For each internal node, if removing that subtree and replacing with majority class doesn't affect accuracy on validation set it can be pruned\n",
    "- Leaf nodes with low confidence can be replace with parent's majority or mark as uncertain\n",
    "- The most important features can be kept and rules that split on less important features in the tree can be pruned\n",
    "- Pruning decisions should be based on: validation set results, instance counts at each node, classification confidence for leaf nodes\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- The odor feature alone provides strong and reliable baseline performance.\n",
    "- Adding more features beyond odor may provide marginal improvements, but risks overfitting on the training data.\n",
    "- The subset with the highest accuracy balances predictive power with model simplicity.\n",
    "\n",
    "- Pruning the decision tree would:\n",
    "   - Reduce overfitting\n",
    "   - Improve generalization \n",
    "   - Create more interpretable rules\n",
    "   - Keep high accuracy with reduced complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b3c60cb-2170-4c9a-a02a-b3eabdb914f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_change(original_accuracy, modified_accuracy, \n",
    "                  original_leaves, modified_leaves,\n",
    "                  original_stats, modified_stats):\n",
    "    \"\"\"\n",
    "    Report the improvement/difference before and after optimization.\n",
    "    \n",
    "    Args:\n",
    "        original_accuracy: Accuracy before optimization\n",
    "        modified_accuracy: Accuracy after optimization\n",
    "        original_leaves: Number of leaves before optimization\n",
    "        modified_leaves: Number of leaves after optimization\n",
    "        original_stats: Statistics dict for original tree\n",
    "        modified_stats: Statistics dict for modified tree\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tree complexity comparison\n",
    "    print(\"\\nTREE COMPLEXITY\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Metric':<30} {'Original':>15} {'Modified':>15} {'Change':>15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    leaves_reduction = original_leaves - modified_leaves\n",
    "    leaves_pct = (leaves_reduction / original_leaves * 100) if original_leaves > 0 else 0\n",
    "    print(f\"{'Number of Leaf Nodes':<30} {original_leaves:>15} {modified_leaves:>15} \"\n",
    "          f\"{-leaves_reduction:>15} ({-leaves_pct:.1f}%)\")\n",
    "    \n",
    "    orig_avg = sum(original_stats['counts']) / len(original_stats['counts'])\n",
    "    modified_avg = sum(modified_stats['counts']) / len(modified_stats['counts'])\n",
    "    print(f\"{'Avg Instances per Leaf':<30} {orig_avg:>15.1f} {modified_avg:>15.1f} \"\n",
    "          f\"{modified_avg - orig_avg:>15.1f}\")\n",
    "\n",
    "    # Performance comparison\n",
    "    print(\"\\nPREDICTIVE PERFORMANCE\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Metric':<30} {'Original':>15} {'Modified':>15} {'Change':>15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    orig_error = 1 - original_accuracy\n",
    "    modified_error = 1 - modified_accuracy\n",
    "    accuracy_diff = modified_accuracy - original_accuracy\n",
    "    error_diff = modified_error - orig_error\n",
    "    \n",
    "    print(f\"{'Accuracy':<30} {original_accuracy:>14.4f} {modified_accuracy:>14.4f} \"\n",
    "          f\"{accuracy_diff:>+14.4f}\")\n",
    "    print(f\"{'Error Rate':<30} {orig_error:>14.4f} {modified_error:>14.4f} \"\n",
    "          f\"{error_diff:>+14.4f}\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otrylzqkgwq",
   "metadata": {},
   "source": [
    "## Pruning by Removing Rules with Few Instances\n",
    "\n",
    "Implementing pruning to remove rules with very few training instances and replace them with majority class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "kcoccs7xpr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruning with min_instances = 15\n",
      "\n",
      "================================================================================\n",
      "PRUNED TREE EVALUATION\n",
      "================================================================================\n",
      "\n",
      "TREE COMPLEXITY\n",
      "--------------------------------------------------------------------------------\n",
      "Metric                                Original        Modified          Change\n",
      "--------------------------------------------------------------------------------\n",
      "Number of Leaf Nodes                        59              59               0 (-0.0%)\n",
      "Avg Instances per Leaf                   123.9           123.9             0.0\n",
      "\n",
      "PREDICTIVE PERFORMANCE\n",
      "--------------------------------------------------------------------------------\n",
      "Metric                                Original        Modified          Change\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy                               1.0000         0.9990        -0.0010\n",
      "Error Rate                             0.0000         0.0010        +0.0010\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "NEW DECISION RULES\n",
      "================================================================================\n",
      "\n",
      "IF odor == a THEN class = e\n",
      "IF odor == c THEN class = p\n",
      "IF odor == f THEN class = p\n",
      "IF odor == l THEN class = e\n",
      "IF odor == m THEN class = p\n",
      "IF odor == n AND cap-color == b AND population == a THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == c THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == n THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == s THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == v THEN class = p\n",
      "IF odor == n AND cap-color == b AND population == y THEN class = e\n",
      "IF odor == n AND cap-color == c THEN class = e\n",
      "IF odor == n AND cap-color == e THEN class = e\n",
      "IF odor == n AND cap-color == g THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == b THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == c THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == e THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == g THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == a THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == c THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == n THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == s THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == v THEN class = p\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == y THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == g THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == l THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == m THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == p THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == u THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == w THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == o THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == p THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == w THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == y THEN class = p\n",
      "IF odor == n AND cap-color == p AND habitat == d THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == g THEN class = p\n",
      "IF odor == n AND cap-color == p AND habitat == l THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == m THEN class = p\n",
      "IF odor == n AND cap-color == p AND habitat == p THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == u THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == w THEN class = e\n",
      "IF odor == n AND cap-color == r THEN class = e\n",
      "IF odor == n AND cap-color == u THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == a THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == c THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == n THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == s THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == d THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == g THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == l THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == m THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == p THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == u THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == w THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == y THEN class = e\n",
      "IF odor == n AND cap-color == y THEN class = p\n",
      "IF odor == p THEN class = p\n",
      "IF odor == s THEN class = p\n",
      "IF odor == y THEN class = p\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def prune_tree(tree, min_instances=10):\n",
    "    \"\"\"Prune decision tree by removing leaves with few instances.\n",
    "    \n",
    "    Args:\n",
    "        tree: The decision tree to prune (nested list format)\n",
    "        min_instances: Minimum number of instances required to keep a leaf\n",
    "        \n",
    "    Returns:\n",
    "        Pruned tree\n",
    "    \"\"\"\n",
    "    if tree[0] == \"Leaf\":\n",
    "        # If it's a leaf, return as-is (pruning happens at parent level)\n",
    "        return tree\n",
    "    \n",
    "    if tree[0] == \"Attribute\":\n",
    "        # Process each value branch\n",
    "        pruned_tree = [\"Attribute\", tree[1]]\n",
    "        \n",
    "        # Get all leaves from children to determine if should prune\n",
    "        leaves = []\n",
    "        for i in range(2, len(tree)):\n",
    "            value_branch = tree[i]  # [\"Value\", value, subtree]\n",
    "            if value_branch[0] == \"Value\":\n",
    "                value = value_branch[1]\n",
    "                subtree = value_branch[2]\n",
    "                \n",
    "                # Recursively prune subtree\n",
    "                pruned_subtree = prune_tree(subtree, min_instances)\n",
    "                \n",
    "                # Get leaf information\n",
    "                if pruned_subtree[0] == \"Leaf\":\n",
    "                    leaves.append({\n",
    "                        'label': pruned_subtree[1],\n",
    "                        'count': pruned_subtree[2],\n",
    "                        'parent_count': pruned_subtree[3]\n",
    "                    })\n",
    "                \n",
    "                pruned_tree.append([\"Value\", value, pruned_subtree])\n",
    "        \n",
    "        # Check if all children are leaves with low instance counts\n",
    "        if leaves and len(leaves) == len(tree) - 2:  # -2 for \"Attribute\" and attribute name\n",
    "            total_instances = sum(leaf['count'] for leaf in leaves)\n",
    "            \n",
    "            # If instances below threshold, collapse to one leaf\n",
    "            if total_instances < min_instances:\n",
    "                # Find majority class of the leaves\n",
    "                label_counts = {}\n",
    "                for leaf in leaves:\n",
    "                    label = leaf['label']\n",
    "                    label_counts[label] = label_counts.get(label, 0) + leaf['count']\n",
    "                \n",
    "                majority_label = max(label_counts.items(), key=lambda x: x[1])[0]\n",
    "                \n",
    "                # Return collapsed leaf (use first leaf's parent_count)\n",
    "                return [\"Leaf\", majority_label, total_instances, leaves[0]['parent_count']]\n",
    "        \n",
    "        return pruned_tree\n",
    "    \n",
    "    return tree\n",
    "\n",
    "\n",
    "def count_leaves(tree):\n",
    "    \"\"\"Count the number of leaf nodes in a tree.\"\"\"\n",
    "    if tree[0] == \"Leaf\":\n",
    "        return 1\n",
    "    elif tree[0] == \"Attribute\":\n",
    "        count = 0\n",
    "        for i in range(2, len(tree)):\n",
    "            value_branch = tree[i]\n",
    "            if value_branch[0] == \"Value\":\n",
    "                count += count_leaves(value_branch[2])\n",
    "        return count\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_leaf_statistics(tree, stats=None):\n",
    "    \"\"\"Get statistics about leaf nodes.\"\"\"\n",
    "    if stats is None:\n",
    "        stats = {'counts': [], 'labels': []}\n",
    "    \n",
    "    if tree[0] == \"Leaf\":\n",
    "        stats['counts'].append(tree[2])\n",
    "        stats['labels'].append(tree[1])\n",
    "    elif tree[0] == \"Attribute\":\n",
    "        for i in range(2, len(tree)):\n",
    "            value_branch = tree[i]\n",
    "            if value_branch[0] == \"Value\":\n",
    "                get_leaf_statistics(value_branch[2], stats)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "original_tree = copy.deepcopy(best_result['tree'].tree)\n",
    "original_leaves = count_leaves(original_tree)\n",
    "original_stats = get_leaf_statistics(original_tree)\n",
    "\n",
    "# Apply pruning with min_instances threshold\n",
    "MIN_INSTANCES = 15\n",
    "print(f\"\\nPruning with min_instances = {MIN_INSTANCES}\")\n",
    "pruned_tree_obj = copy.deepcopy(best_result['tree'])\n",
    "pruned_tree_obj.tree = prune_tree(original_tree, min_instances=MIN_INSTANCES)\n",
    "pruned_leaves = count_leaves(pruned_tree_obj.tree)\n",
    "pruned_stats = get_leaf_statistics(pruned_tree_obj.tree)\n",
    "\n",
    "# Evaluate pruned tree\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PRUNED TREE EVALUATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Evaluate on same data using cross-validation\n",
    "folds = myevaluation.kfold_split(X_best, n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for train_indices, test_indices in folds:\n",
    "    X_train_fold = [X_best[i] for i in train_indices]\n",
    "    y_train_fold = [y[i] for i in train_indices]\n",
    "    X_test_fold = [X_best[i] for i in test_indices]\n",
    "    y_test_fold = [y[i] for i in test_indices]\n",
    "    \n",
    "    # Train and prune\n",
    "    tree = MyDecisionTreeClassifier()\n",
    "    tree.fit(X_train_fold, y_train_fold)\n",
    "    tree.tree = prune_tree(tree.tree, min_instances=MIN_INSTANCES)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = tree.predict(X_test_fold)\n",
    "    \n",
    "    all_y_true.extend(y_test_fold)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "# Compute metrics\n",
    "pruned_accuracy = myevaluation.accuracy_score(all_y_true, all_y_pred)\n",
    "\n",
    "# Call the improvement report function\n",
    "report_change(\n",
    "    best_result['accuracy'], \n",
    "    pruned_accuracy,\n",
    "    original_leaves,\n",
    "    pruned_leaves,\n",
    "    original_stats,\n",
    "    pruned_stats\n",
    ")\n",
    "\n",
    "# Print pruned decision rules\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"NEW DECISION RULES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "pruned_tree_obj.print_decision_rules(attribute_names=best_result['features'], class_name='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mm289baulr",
   "metadata": {},
   "source": [
    "## Rule Merging: Combining Similar Rules with Same Outcome\n",
    "\n",
    "Implementing rule merging to simplify the tree by combining multiple branches that lead to the same classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7g96njbdra3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merge threshold: 80% (branches must agree this much to merge)\n",
      "\n",
      "Original Tree:\n",
      "  Decision rules: 59\n",
      "  Leaf nodes: 59\n",
      "\n",
      "Merged Tree:\n",
      "  Decision rules: 59 (reduced by 0)\n",
      "  Leaf nodes: 59 (reduced by 0)\n",
      "  Avg instances per leaf: 123.9\n",
      "\n",
      "================================================================================\n",
      "MERGED TREE EVALUATION\n",
      "================================================================================\n",
      "\n",
      "TREE COMPLEXITY\n",
      "--------------------------------------------------------------------------------\n",
      "Metric                                Original        Modified          Change\n",
      "--------------------------------------------------------------------------------\n",
      "Number of Leaf Nodes                        59              59               0 (-0.0%)\n",
      "Avg Instances per Leaf                   123.9           123.9             0.0\n",
      "\n",
      "PREDICTIVE PERFORMANCE\n",
      "--------------------------------------------------------------------------------\n",
      "Metric                                Original        Modified          Change\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy                               1.0000         0.9998        -0.0002\n",
      "Error Rate                             0.0000         0.0002        +0.0002\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "NEW DECISION RULES\n",
      "================================================================================\n",
      "\n",
      "IF odor == a THEN class = e\n",
      "IF odor == c THEN class = p\n",
      "IF odor == f THEN class = p\n",
      "IF odor == l THEN class = e\n",
      "IF odor == m THEN class = p\n",
      "IF odor == n AND cap-color == b AND population == a THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == c THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == n THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == s THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == v THEN class = p\n",
      "IF odor == n AND cap-color == b AND population == y THEN class = e\n",
      "IF odor == n AND cap-color == c THEN class = e\n",
      "IF odor == n AND cap-color == e THEN class = e\n",
      "IF odor == n AND cap-color == g THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == b THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == c THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == e THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == g THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == a THEN class = p\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == c THEN class = p\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == n THEN class = p\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == s THEN class = p\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == v THEN class = p\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == y THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == g THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == l THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == m THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == p THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == u THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == w THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == o THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == p THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == w THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == y THEN class = p\n",
      "IF odor == n AND cap-color == p AND habitat == d THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == g THEN class = p\n",
      "IF odor == n AND cap-color == p AND habitat == l THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == m THEN class = p\n",
      "IF odor == n AND cap-color == p AND habitat == p THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == u THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == w THEN class = e\n",
      "IF odor == n AND cap-color == r THEN class = e\n",
      "IF odor == n AND cap-color == u THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == a THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == c THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == n THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == s THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == d THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == g THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == l THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == m THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == p THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == u THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == w THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == y THEN class = e\n",
      "IF odor == n AND cap-color == y THEN class = p\n",
      "IF odor == p THEN class = p\n",
      "IF odor == s THEN class = p\n",
      "IF odor == y THEN class = p\n"
     ]
    }
   ],
   "source": [
    "def merge_similar_rules(tree, merge_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Merge rules that lead to the same class with similar conditions.\n",
    "    \n",
    "    If a majority (>= merge_threshold) of branches from an attribute node lead to the same class,\n",
    "    collapse that node to a single leaf with that class.\n",
    "    \n",
    "    Args:\n",
    "        tree: The decision tree to simplify (nested list format)\n",
    "        merge_threshold: Fraction of branches that must agree (0.0 to 1.0)\n",
    "                        Default 0.8 means 80% of branches must lead to same class\n",
    "    \n",
    "    Returns:\n",
    "        Simplified tree with merged rules\n",
    "    \"\"\"\n",
    "    if tree[0] == \"Leaf\":\n",
    "        return tree\n",
    "    \n",
    "    if tree[0] == \"Attribute\":\n",
    "        # First, recursively process all children\n",
    "        merged_tree = [\"Attribute\", tree[1]]\n",
    "        child_leaves = []\n",
    "        \n",
    "        for i in range(2, len(tree)):\n",
    "            value_branch = tree[i]\n",
    "            if value_branch[0] == \"Value\":\n",
    "                value = value_branch[1]\n",
    "                subtree = value_branch[2]\n",
    "                \n",
    "                # Recursively merge the subtree\n",
    "                merged_subtree = merge_similar_rules(subtree, merge_threshold)\n",
    "                merged_tree.append([\"Value\", value, merged_subtree])\n",
    "                \n",
    "                # Collect leaf information\n",
    "                if merged_subtree[0] == \"Leaf\":\n",
    "                    child_leaves.append({\n",
    "                        'label': merged_subtree[1],\n",
    "                        'count': merged_subtree[2],\n",
    "                        'parent_count': merged_subtree[3]\n",
    "                    })\n",
    "        \n",
    "        # Check if we can merge based on class agreement\n",
    "        if child_leaves and len(child_leaves) == len(tree) - 2:  # All children are leaves\n",
    "            # Count instances by label\n",
    "            label_counts = {}\n",
    "            total_instances = 0\n",
    "            \n",
    "            for leaf in child_leaves:\n",
    "                label = leaf['label']\n",
    "                count = leaf['count']\n",
    "                label_counts[label] = label_counts.get(label, 0) + count\n",
    "                total_instances += count\n",
    "            \n",
    "            # Find the majority class\n",
    "            if label_counts:\n",
    "                majority_label = max(label_counts.items(), key=lambda x: x[1])[0]\n",
    "                majority_count = label_counts[majority_label]\n",
    "                \n",
    "                # Check if majority exceeds threshold\n",
    "                if total_instances > 0:\n",
    "                    majority_fraction = majority_count / total_instances\n",
    "                    \n",
    "                    if majority_fraction >= merge_threshold:\n",
    "                        # Merge into a single leaf\n",
    "                        return [\"Leaf\", majority_label, total_instances, child_leaves[0]['parent_count']]\n",
    "        \n",
    "        return merged_tree\n",
    "    \n",
    "    return tree\n",
    "\n",
    "\n",
    "def count_rules(tree, path=[]):\n",
    "    \"\"\"Count the number of decision rules (leaf nodes) in a tree.\"\"\"\n",
    "    if tree[0] == \"Leaf\":\n",
    "        return 1\n",
    "    elif tree[0] == \"Attribute\":\n",
    "        total = 0\n",
    "        for i in range(2, len(tree)):\n",
    "            value_branch = tree[i]\n",
    "            if value_branch[0] == \"Value\":\n",
    "                total += count_rules(value_branch[2])\n",
    "        return total\n",
    "    return 0\n",
    "\n",
    "original_tree_copy = copy.deepcopy(best_result['tree'].tree)\n",
    "\n",
    "MERGE_THRESHOLD = 0.8\n",
    "print(f\"\\nMerge threshold: {MERGE_THRESHOLD*100:.0f}% (branches must agree this much to merge)\")\n",
    "\n",
    "# Create merged tree\n",
    "merged_tree_obj = copy.deepcopy(best_result['tree'])\n",
    "merged_tree_obj.tree = merge_similar_rules(original_tree_copy, merge_threshold=MERGE_THRESHOLD)\n",
    "\n",
    "# Count rules before and after\n",
    "original_rules = count_rules(original_tree_copy)\n",
    "merged_rules = count_rules(merged_tree_obj.tree)\n",
    "merged_leaves = count_leaves(merged_tree_obj.tree)\n",
    "merged_stats = get_leaf_statistics(merged_tree_obj.tree)\n",
    "\n",
    "print(f\"\\nOriginal Tree:\")\n",
    "print(f\"  Decision rules: {original_rules}\")\n",
    "print(f\"  Leaf nodes: {original_leaves}\")\n",
    "\n",
    "print(f\"\\nMerged Tree:\")\n",
    "print(f\"  Decision rules: {merged_rules} (reduced by {original_rules - merged_rules})\")\n",
    "print(f\"  Leaf nodes: {merged_leaves} (reduced by {original_leaves - merged_leaves})\")\n",
    "if merged_stats['counts']:\n",
    "    print(f\"  Avg instances per leaf: {sum(merged_stats['counts'])/len(merged_stats['counts']):.1f}\")\n",
    "\n",
    "# Evaluate merged tree\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MERGED TREE EVALUATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "folds = myevaluation.kfold_split(X_best, n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for train_indices, test_indices in folds:\n",
    "    X_train_fold = [X_best[i] for i in train_indices]\n",
    "    y_train_fold = [y[i] for i in train_indices]\n",
    "    X_test_fold = [X_best[i] for i in test_indices]\n",
    "    y_test_fold = [y[i] for i in test_indices]\n",
    "    \n",
    "    # Train and merge\n",
    "    tree = MyDecisionTreeClassifier()\n",
    "    tree.fit(X_train_fold, y_train_fold)\n",
    "    tree.tree = merge_similar_rules(tree.tree, merge_threshold=MERGE_THRESHOLD)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = tree.predict(X_test_fold)\n",
    "    \n",
    "    all_y_true.extend(y_test_fold)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "# Compute metrics\n",
    "merged_accuracy = myevaluation.accuracy_score(all_y_true, all_y_pred)\n",
    "\n",
    "# Use the reporting function\n",
    "report_change(\n",
    "    best_result['accuracy'], \n",
    "    merged_accuracy,\n",
    "    original_leaves,\n",
    "    merged_leaves,\n",
    "    original_stats,\n",
    "    merged_stats\n",
    ")\n",
    "\n",
    "# Print decision rules\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"NEW DECISION RULES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "final_tree.print_decision_rules(attribute_names=best_result['features'], class_name='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dzmmkd6uoaj",
   "metadata": {},
   "source": [
    "## Validation-Based Pruning\n",
    "\n",
    "Testing each internal node by seeing if replacing the subtree with majority class doesn't hurt validation accuracy and pruning in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "g0b0nw5kot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VALIDATION-BASED PRUNING\n",
      "================================================================================\n",
      "\n",
      "TREE COMPLEXITY\n",
      "--------------------------------------------------------------------------------\n",
      "Metric                                Original        Modified          Change\n",
      "--------------------------------------------------------------------------------\n",
      "Number of Leaf Nodes                        59              59               0 (-0.0%)\n",
      "Avg Instances per Leaf                   123.9           123.9             0.0\n",
      "\n",
      "PREDICTIVE PERFORMANCE\n",
      "--------------------------------------------------------------------------------\n",
      "Metric                                Original        Modified          Change\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy                               1.0000         1.0000        +0.0000\n",
      "Error Rate                             0.0000         0.0000        +0.0000\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "NEW DECISION RULES\n",
      "================================================================================\n",
      "\n",
      "IF odor == a THEN class = e\n",
      "IF odor == c THEN class = p\n",
      "IF odor == f THEN class = p\n",
      "IF odor == l THEN class = e\n",
      "IF odor == m THEN class = p\n",
      "IF odor == n AND cap-color == b AND population == a THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == c THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == n THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == s THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == v THEN class = p\n",
      "IF odor == n AND cap-color == b AND population == y THEN class = e\n",
      "IF odor == n AND cap-color == c THEN class = e\n",
      "IF odor == n AND cap-color == e THEN class = e\n",
      "IF odor == n AND cap-color == g THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == b THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == c THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == e THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == g THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == a THEN class = p\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == c THEN class = p\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == n THEN class = p\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == s THEN class = p\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == v THEN class = p\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == d AND population == y THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == g THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == l THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == m THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == p THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == u THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == n AND habitat == w THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == o THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == p THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == w THEN class = e\n",
      "IF odor == n AND cap-color == n AND stalk-color-below-ring == y THEN class = p\n",
      "IF odor == n AND cap-color == p AND habitat == d THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == g THEN class = p\n",
      "IF odor == n AND cap-color == p AND habitat == l THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == m THEN class = p\n",
      "IF odor == n AND cap-color == p AND habitat == p THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == u THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == w THEN class = e\n",
      "IF odor == n AND cap-color == r THEN class = e\n",
      "IF odor == n AND cap-color == u THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == a THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == c THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == n THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == s THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == d THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == g THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == l THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == m THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == p THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == u THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == w THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == y THEN class = e\n",
      "IF odor == n AND cap-color == y THEN class = p\n",
      "IF odor == p THEN class = p\n",
      "IF odor == s THEN class = p\n",
      "IF odor == y THEN class = p\n"
     ]
    }
   ],
   "source": [
    "def validation_prune_tree(tree, X_val, y_val, tree_obj):\n",
    "    \"\"\"\n",
    "    Prune tree using validation set - replace subtrees with majority class if accuracy doesn't decrease.\n",
    "    \n",
    "    This is reduced error pruning: for each internal node, test if replacing it with a leaf\n",
    "    improves or maintains validation accuracy.\n",
    "    \n",
    "    Args:\n",
    "        tree: The decision tree structure (nested list)\n",
    "        X_val: Validation features\n",
    "        y_val: Validation labels\n",
    "        tree_obj: The tree object (for prediction)\n",
    "    \n",
    "    Returns:\n",
    "        Pruned tree\n",
    "    \"\"\"\n",
    "    # Get baseline accuracy with current tree\n",
    "    baseline_predictions = tree_obj.predict(X_val)\n",
    "    baseline_accuracy = myevaluation.accuracy_score(y_val, baseline_predictions)\n",
    "    \n",
    "    # Try pruning this node\n",
    "    pruned_tree = _validation_prune_recursive(tree, X_val, y_val, tree_obj, baseline_accuracy)\n",
    "    \n",
    "    return pruned_tree\n",
    "\n",
    "\n",
    "def _validation_prune_recursive(tree, X_val, y_val, tree_obj, baseline_accuracy):\n",
    "    \"\"\"\n",
    "    Recursively test pruning each subtree against validation set.\n",
    "    \"\"\"\n",
    "    if tree[0] == \"Leaf\":\n",
    "        return tree\n",
    "    \n",
    "    if tree[0] == \"Attribute\":\n",
    "        # First, recursively prune children\n",
    "        pruned_tree = [\"Attribute\", tree[1]]\n",
    "        \n",
    "        for i in range(2, len(tree)):\n",
    "            value_branch = tree[i]\n",
    "            if value_branch[0] == \"Value\":\n",
    "                value = value_branch[1]\n",
    "                subtree = value_branch[2]\n",
    "                \n",
    "                # Recursively prune the subtree\n",
    "                pruned_subtree = _validation_prune_recursive(subtree, X_val, y_val, tree_obj, baseline_accuracy)\n",
    "                pruned_tree.append([\"Value\", value, pruned_subtree])\n",
    "        \n",
    "        # Now test if we should prune this entire node\n",
    "        # Collect all leaf labels under this node\n",
    "        leaf_labels = []\n",
    "        _collect_leaf_labels(pruned_tree, leaf_labels)\n",
    "        \n",
    "        if leaf_labels:\n",
    "            # Find majority class\n",
    "            label_counts = {}\n",
    "            for label in leaf_labels:\n",
    "                label_counts[label] = label_counts.get(label, 0) + 1\n",
    "            majority_label = max(label_counts.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "            # Create a version with this node replaced by a leaf\n",
    "            test_tree = [\"Leaf\", majority_label, sum(label_counts.values()), sum(label_counts.values())]\n",
    "            \n",
    "            # Test accuracy with pruned version\n",
    "            original_tree = tree_obj.tree\n",
    "            tree_obj.tree = test_tree\n",
    "            pruned_predictions = tree_obj.predict(X_val)\n",
    "            pruned_accuracy = myevaluation.accuracy_score(y_val, pruned_predictions)\n",
    "            tree_obj.tree = original_tree  # Restore\n",
    "            \n",
    "            # If accuracy doesn't decrease (or improves), prune it\n",
    "            if pruned_accuracy >= baseline_accuracy:\n",
    "                return test_tree\n",
    "        \n",
    "        return pruned_tree\n",
    "    \n",
    "    return tree\n",
    "\n",
    "\n",
    "def _collect_leaf_labels(tree, labels):\n",
    "    \"\"\"Collect all leaf labels from a tree.\"\"\"\n",
    "    if tree[0] == \"Leaf\":\n",
    "        labels.append(tree[1])\n",
    "    elif tree[0] == \"Attribute\":\n",
    "        for i in range(2, len(tree)):\n",
    "            value_branch = tree[i]\n",
    "            if value_branch[0] == \"Value\":\n",
    "                _collect_leaf_labels(value_branch[2], labels)\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDATION-BASED PRUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use k-fold cross-validation\n",
    "folds = myevaluation.kfold_split(X_best, n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "prune_counts = []\n",
    "\n",
    "for fold_idx, (train_indices, val_indices) in enumerate(folds):\n",
    "    # Split into train and validation\n",
    "    X_train_fold = [X_best[i] for i in train_indices]\n",
    "    y_train_fold = [y[i] for i in train_indices]\n",
    "    X_val_fold = [X_best[i] for i in val_indices]\n",
    "    y_val_fold = [y[i] for i in val_indices]\n",
    "    \n",
    "    # Train tree\n",
    "    tree = MyDecisionTreeClassifier()\n",
    "    tree.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Count leaves before pruning\n",
    "    leaves_before = count_leaves(tree.tree)\n",
    "    \n",
    "    # Apply validation-based pruning\n",
    "    tree.tree = validation_prune_tree(tree.tree, X_val_fold, y_val_fold, tree)\n",
    "    \n",
    "    # Count leaves after pruning\n",
    "    leaves_after = count_leaves(tree.tree)\n",
    "    prune_counts.append(leaves_before - leaves_after)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred = tree.predict(X_val_fold)\n",
    "    \n",
    "    all_y_true.extend(y_val_fold)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "# Compute overall metrics\n",
    "validation_pruned_accuracy = myevaluation.accuracy_score(all_y_true, all_y_pred)\n",
    "\n",
    "# Get statistics about a single validation-pruned tree for comparison\n",
    "final_tree = MyDecisionTreeClassifier()\n",
    "final_tree.fit(X_best, y)\n",
    "\n",
    "# Split for validation (use 10% as validation)\n",
    "from random import sample\n",
    "val_size = len(X_best) // 10\n",
    "val_indices = sample(range(len(X_best)), val_size)\n",
    "train_indices = [i for i in range(len(X_best)) if i not in val_indices]\n",
    "\n",
    "X_train_final = [X_best[i] for i in train_indices]\n",
    "y_train_final = [y[i] for i in train_indices]\n",
    "X_val_final = [X_best[i] for i in val_indices]\n",
    "y_val_final = [y[i] for i in val_indices]\n",
    "\n",
    "final_tree.fit(X_train_final, y_train_final)\n",
    "original_leaves_final = count_leaves(final_tree.tree)\n",
    "final_tree.tree = validation_prune_tree(final_tree.tree, X_val_final, y_val_final, final_tree)\n",
    "validation_pruned_leaves = count_leaves(final_tree.tree)\n",
    "validation_pruned_stats = get_leaf_statistics(final_tree.tree)\n",
    "\n",
    "report_change(\n",
    "    best_result['accuracy'],\n",
    "    validation_pruned_accuracy,\n",
    "    original_leaves,\n",
    "    validation_pruned_leaves,\n",
    "    original_stats,\n",
    "    validation_pruned_stats\n",
    ")\n",
    "\n",
    "# Print decision rules\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"NEW DECISION RULES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "final_tree.print_decision_rules(attribute_names=best_result['features'], class_name='class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "q4kxcao5ude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMBINED OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "Tree Complexity:\n",
      "  Original:   59 leaves\n",
      "  Optimized:  40 leaves\n",
      "  Reduction:  19 leaves (32.2%)\n",
      "\n",
      "TREE COMPLEXITY\n",
      "--------------------------------------------------------------------------------\n",
      "Metric                                Original        Modified          Change\n",
      "--------------------------------------------------------------------------------\n",
      "Number of Leaf Nodes                        59              40             -19 (-32.2%)\n",
      "Avg Instances per Leaf                   123.9           182.8            58.9\n",
      "\n",
      "PREDICTIVE PERFORMANCE\n",
      "--------------------------------------------------------------------------------\n",
      "Metric                                Original        Modified          Change\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy                               1.0000         0.9972        -0.0028\n",
      "Error Rate                             0.0000         0.0028        +0.0028\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "NEW DECISION RULES\n",
      "================================================================================\n",
      "\n",
      "IF odor == a THEN class = e\n",
      "IF odor == c THEN class = p\n",
      "IF odor == f THEN class = p\n",
      "IF odor == l THEN class = e\n",
      "IF odor == m THEN class = p\n",
      "IF odor == n AND cap-color == b AND population == a THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == c THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == n THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == s THEN class = e\n",
      "IF odor == n AND cap-color == b AND population == v THEN class = p\n",
      "IF odor == n AND cap-color == b AND population == y THEN class = e\n",
      "IF odor == n AND cap-color == c THEN class = e\n",
      "IF odor == n AND cap-color == e THEN class = e\n",
      "IF odor == n AND cap-color == g THEN class = e\n",
      "IF odor == n AND cap-color == n THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == d THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == g THEN class = p\n",
      "IF odor == n AND cap-color == p AND habitat == l THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == m THEN class = p\n",
      "IF odor == n AND cap-color == p AND habitat == p THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == u THEN class = e\n",
      "IF odor == n AND cap-color == p AND habitat == w THEN class = e\n",
      "IF odor == n AND cap-color == r THEN class = e\n",
      "IF odor == n AND cap-color == u THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == a THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == c THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == n THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == s THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == d THEN class = e\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == g THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == l THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == m THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == p THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == u THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == v AND habitat == w THEN class = p\n",
      "IF odor == n AND cap-color == w AND population == y THEN class = e\n",
      "IF odor == n AND cap-color == y THEN class = p\n",
      "IF odor == p THEN class = p\n",
      "IF odor == s THEN class = p\n",
      "IF odor == y THEN class = p\n",
      "\n",
      "Technique                              Leaves     Accuracy \n",
      "---------------------------------------------------------------------------\n",
      "Original Tree                               59       1.0000 \n",
      "Instance-based Pruning                      59       0.9990 \n",
      "Rule Merging                                59       0.9998 \n",
      "Validation-based Pruning                    59       1.0000 \n",
      "Full Pipeline                               40       0.9972 \n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def optimize_tree(tree, tree_obj, X_val=None, y_val=None, min_instances=15, merge_threshold=0.8, use_validation_pruning=True):\n",
    "    \"\"\"\n",
    "    Apply validation-based pruning, instance-based pruning, and rule merging for optimal tree simplification.\n",
    "    \n",
    "    Args:\n",
    "        tree: The decision tree to optimize\n",
    "        tree_obj: The tree classifier object (needed for validation pruning)\n",
    "        X_val: Validation features (optional, needed for validation pruning)\n",
    "        y_val: Validation labels (optional, needed for validation pruning)\n",
    "        min_instances: Minimum instances for instance-based pruning\n",
    "        merge_threshold: Agreement threshold for merging\n",
    "        use_validation_pruning: Whether to apply validation-based pruning\n",
    "    \n",
    "    Returns:\n",
    "        Optimized tree\n",
    "    \"\"\"\n",
    "    current_tree = tree\n",
    "    \n",
    "    # Validation-based pruning\n",
    "    current_tree = validation_prune_tree(current_tree, X_val, y_val, tree_obj)\n",
    "    \n",
    "    # Apply instance-based pruning (remove low-instance rules)\n",
    "    current_tree = prune_tree(current_tree, min_instances=min_instances)\n",
    "    \n",
    "    # Apply rule merging \n",
    "    current_tree = merge_similar_rules(current_tree, merge_threshold=merge_threshold)\n",
    "    \n",
    "    return current_tree\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMBINED OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use cross-validation to evaluate the full optimization pipeline\n",
    "folds = myevaluation.kfold_split(X_best, n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for train_indices, test_indices in folds:\n",
    "    X_train_fold = [X_best[i] for i in train_indices]\n",
    "    y_train_fold = [y[i] for i in train_indices]\n",
    "    X_test_fold = [X_best[i] for i in test_indices]\n",
    "    y_test_fold = [y[i] for i in test_indices]\n",
    "    \n",
    "    # Further split training into train/validation for validation pruning\n",
    "    val_size = len(X_train_fold) // 10\n",
    "    val_indices_inner = list(range(len(X_train_fold)))[:val_size]\n",
    "    train_indices_inner = list(range(len(X_train_fold)))[val_size:]\n",
    "    \n",
    "    X_train_inner = [X_train_fold[i] for i in train_indices_inner]\n",
    "    y_train_inner = [y_train_fold[i] for i in train_indices_inner]\n",
    "    X_val_inner = [X_train_fold[i] for i in val_indices_inner]\n",
    "    y_val_inner = [y_train_fold[i] for i in val_indices_inner]\n",
    "    \n",
    "    # Train tree on inner training set\n",
    "    tree = MyDecisionTreeClassifier()\n",
    "    tree.fit(X_train_inner, y_train_inner)\n",
    "    \n",
    "    # Apply full optimization pipeline with validation data\n",
    "    tree.tree = optimize_tree(\n",
    "        tree.tree, \n",
    "        tree,\n",
    "        X_val=X_val_inner,\n",
    "        y_val=y_val_inner,\n",
    "        min_instances=MIN_INSTANCES, \n",
    "        merge_threshold=MERGE_THRESHOLD,\n",
    "        use_validation_pruning=True\n",
    "    )\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = tree.predict(X_test_fold)\n",
    "    \n",
    "    all_y_true.extend(y_test_fold)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "# Compute metrics\n",
    "full_optimized_accuracy = myevaluation.accuracy_score(all_y_true, all_y_pred)\n",
    "\n",
    "# Create a single optimized tree for display\n",
    "from random import sample\n",
    "val_size = len(X_best) // 10\n",
    "val_indices = sample(range(len(X_best)), val_size)\n",
    "train_indices = [i for i in range(len(X_best)) if i not in val_indices]\n",
    "\n",
    "X_train_display = [X_best[i] for i in train_indices]\n",
    "y_train_display = [y[i] for i in train_indices]\n",
    "X_val_display = [X_best[i] for i in val_indices]\n",
    "y_val_display = [y[i] for i in val_indices]\n",
    "\n",
    "display_tree = MyDecisionTreeClassifier()\n",
    "display_tree.fit(X_train_display, y_train_display)\n",
    "original_leaves_display = count_leaves(display_tree.tree)\n",
    "original_stats_display = get_leaf_statistics(display_tree.tree)\n",
    "\n",
    "display_tree.tree = optimize_tree(\n",
    "    display_tree.tree,\n",
    "    display_tree,\n",
    "    X_val=X_val_display,\n",
    "    y_val=y_val_display,\n",
    "    min_instances=MIN_INSTANCES,\n",
    "    merge_threshold=MERGE_THRESHOLD,\n",
    "    use_validation_pruning=True\n",
    ")\n",
    "\n",
    "full_optimized_leaves = count_leaves(display_tree.tree)\n",
    "full_optimized_stats = get_leaf_statistics(display_tree.tree)\n",
    "full_optimized_rules = count_rules(display_tree.tree)\n",
    "\n",
    "print(f\"\\nTree Complexity:\")\n",
    "print(f\"  Original:   {original_leaves_display} leaves\")\n",
    "print(f\"  Optimized:  {full_optimized_leaves} leaves\")\n",
    "print(f\"  Reduction:  {original_leaves_display - full_optimized_leaves} leaves ({(original_leaves_display - full_optimized_leaves)/original_leaves_display*100:.1f}%)\")\n",
    "\n",
    "report_change(\n",
    "    best_result['accuracy'], \n",
    "    full_optimized_accuracy,\n",
    "    original_leaves,\n",
    "    full_optimized_leaves,\n",
    "    original_stats,\n",
    "    full_optimized_stats\n",
    ")\n",
    "\n",
    "# Print optimized decision rules\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"NEW DECISION RULES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "display_tree.print_decision_rules(attribute_names=best_result['features'], class_name='class')\n",
    "\n",
    "# Summary comparison of all techniques\n",
    "print(f\"{'\\nTechnique':<35} {'Leaves':>10} {'Accuracy':>12} \")\n",
    "print(\"-\"*75)\n",
    "print(f\"{'Original Tree':<35} {original_leaves:>10} {best_result['accuracy']:>12.4f} \")\n",
    "print(f\"{'Instance-based Pruning':<35} {pruned_leaves:>10} {pruned_accuracy:>12.4f} \")\n",
    "print(f\"{'Rule Merging':<35} {merged_leaves:>10} {merged_accuracy:>12.4f} \")\n",
    "print(f\"{'Validation-based Pruning':<35} {validation_pruned_leaves:>10} {validation_pruned_accuracy:>12.4f} \")\n",
    "print(f\"{'Full Pipeline':<35} {full_optimized_leaves:>10} {full_optimized_accuracy:>12.4f} \")\n",
    "print(\"-\"*75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}